{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alpacaYiChun/ML/blob/master/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-G-BHhdp4FW",
        "outputId": "f9cea83d-cbe8-4b5f-950a-804b546d1a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4GgBQ2w0Vae"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Dense, Embedding, Dropout, LayerNormalization, Input, GlobalAveragePooling1D, Bidirectional, LSTM, Lambda, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import pandas as pd\n",
        "from keras.callbacks import LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atY1EozP0Zrz"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 200\n",
        "NUM_HEAD = 4\n",
        "WORD_EMBED_DIM = 32\n",
        "POS_EMBED_DIM = 32\n",
        "F1 = 64\n",
        "BLOCKS = 2\n",
        "VOCAB_SIZE = 20000  # Only consider the top 20k words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUUkBp500UlU",
        "outputId": "05ff7412-ee9d-4038-ccdc-02fa9629019b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000,) Training sequences\n",
            "(25000,) Validation sequences\n",
            "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
            " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
            " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
            " list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 2, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 2, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 2, 217, 4122, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 2, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 2, 21, 29, 9, 2841, 23, 4, 1010, 2, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 2, 4, 15812, 804, 2, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574])\n",
            " list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])]\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=VOCAB_SIZE)\n",
        "print(x_train.shape, \"Training sequences\")\n",
        "print(x_val.shape, \"Validation sequences\")\n",
        "print(x_train[:5])\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_LEN, padding='post')\n",
        "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=MAX_LEN, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrLgJgOxo_IR"
      },
      "outputs": [],
      "source": [
        "def get_dicts():\n",
        "  word_to_id = tf.keras.datasets.imdb.get_word_index()\n",
        "  wti = {}\n",
        "  itw = {}\n",
        "\n",
        "  wti[\"<PAD>\"] = 0\n",
        "  wti[\"<START>\"] = 1\n",
        "  wti[\"<UNK>\"] = 2\n",
        "  wti[\"<UNUSED>\"] = 3\n",
        "\n",
        "  itw[0] = \"<PAD>\"\n",
        "  itw[1] = \"<CLS>\"\n",
        "  itw[2] = \"<UNK>\"\n",
        "  itw[3] = \"<UNUSED>\"\n",
        "\n",
        "  for k,v in word_to_id.items():\n",
        "    wti[k] = v+3\n",
        "    itw[v+3] = k\n",
        "  return wti, itw\n",
        "\n",
        "wti, itw = get_dicts()\n",
        "\n",
        "def vector_to_sentence(vector):\n",
        "  return \" \".join([itw[v] for v in vector])\n",
        "\n",
        "def sentence_to_vector(str):\n",
        "  words = str.split()\n",
        "  return np.array([wti[w] for w in words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPK6AiAXyap8",
        "outputId": "3d83dff3-dab7-4c5d-abe4-6e19260bef41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 200, 32)      640000      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape (TFOpLambda  (3,)                0           ['embedding[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " tf.tile (TFOpLambda)           (None, 200, 32)      0           ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (None, 200, 64)      0           ['embedding[0][0]',              \n",
            "                                                                  'tf.tile[0][0]']                \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 200, 64)     66368       ['tf.concat[0][0]',              \n",
            " dAttention)                                                      'tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_74 (Dropout)           (None, 200, 64)      0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 200, 64)     0           ['tf.concat[0][0]',              \n",
            " da)                                                              'dropout_74[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 200, 64)     128         ['tf.__operators__.add[0][0]']   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 200, 64)      4160        ['layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 200, 64)      4160        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)           (None, 200, 64)      0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 200, 64)     0           ['layer_normalization[0][0]',    \n",
            " mbda)                                                            'dropout_75[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 200, 64)     128         ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 200, 64)     66368       ['layer_normalization_1[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_76 (Dropout)           (None, 200, 64)      0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 200, 64)     0           ['layer_normalization_1[0][0]',  \n",
            " mbda)                                                            'dropout_76[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 200, 64)     128         ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 200, 64)      4160        ['layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 200, 64)      4160        ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_77 (Dropout)           (None, 200, 64)      0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 200, 64)     0           ['layer_normalization_2[0][0]',  \n",
            " mbda)                                                            'dropout_77[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 200, 64)     128         ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 64)          0           ['layer_normalization_3[0][0]']  \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dropout_78 (Dropout)           (None, 64)           0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 2)            130         ['dropout_78[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 790,018\n",
            "Trainable params: 790,018\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def lr_decay(epoch):\n",
        "  lr_base = 0.0001\n",
        "  decay = pow(0.5, epoch)\n",
        "  return lr_base * decay\n",
        "\n",
        "scheduler = LearningRateScheduler(lr_decay)\n",
        "\n",
        "def lstm(x):\n",
        "  y = x\n",
        "  y = Bidirectional(LSTM(8))(y)\n",
        "  return y\n",
        "\n",
        "def transform(x):\n",
        "  y = x\n",
        "  y = MultiHeadAttention(NUM_HEAD, x.shape[-1])(y, y)\n",
        "  y = Dropout(0.25)(y)\n",
        "  a = LayerNormalization(epsilon=1e-6)(x + y)\n",
        "  y = Dense(F1, activation='relu')(a)\n",
        "  y = Dense(x.shape[-1])(y)\n",
        "  y = Dropout(0.25)(y)\n",
        "  y = LayerNormalization(epsilon=1e-6)(a + y)\n",
        "  return y\n",
        "\n",
        "def embed(x, vocab, max_len):\n",
        "  word_embed = Embedding(vocab, WORD_EMBED_DIM)(x)\n",
        "\n",
        "  all_pos = tf.range(start=0, limit=max_len, delta=1)\n",
        "  pos_embed = Embedding(max_len, POS_EMBED_DIM)(all_pos)\n",
        "\n",
        "  b_expanded = tf.expand_dims(pos_embed, axis=0)\n",
        "  b_tiled = tf.tile(b_expanded, [tf.shape(word_embed)[0], 1, 1])\n",
        "  \n",
        "  return tf.concat([word_embed, b_tiled], axis=-1)\n",
        "\n",
        "def transform_train():\n",
        "  input = Input((MAX_LEN,))\n",
        "  embeded = embed(input, VOCAB_SIZE, MAX_LEN)\n",
        "  transformed = embeded\n",
        "  for i in range(BLOCKS):\n",
        "    transformed = transform(transformed)\n",
        "\n",
        "  ose = GlobalAveragePooling1D()(transformed)\n",
        "  ose = Dropout(0.25)(ose)\n",
        "  ose = Dense(2, activation='softmax')(ose)\n",
        "\n",
        "  model = Model(input, ose)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def tokens_summary(tokens):\n",
        "  ose = GlobalAveragePooling1D()(tokens)\n",
        "  ose = Dropout(0.25)(ose)\n",
        "  ose = Dense(2, activation='softmax')(ose)\n",
        "  return ose  \n",
        "\n",
        "def lstm_train(max_len, vocab, word_embed_dim):\n",
        "  input = Input((max_len,), name=\"input\")\n",
        "  embeded = Embedding(vocab, word_embed_dim, name=\"embed\")(input)\n",
        "  lstms = Bidirectional(LSTM(word_embed_dim, dropout=0.25, recurrent_dropout=0.25), name=\"lstm\")(embeded)\n",
        "  output = Dense(2, activation='softmax', name=\"classify_output\")(lstms)\n",
        "\n",
        "  model = Model(input, output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'], callbacks=[scheduler])\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "#model = lstm_train(MAX_LEN, VOCAB_SIZE, WORD_EMBED_DIM)\n",
        "model = transform_train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN-iOmny8Ez2",
        "outputId": "457c58c9-8af5-4b5e-f56d-0659709b6fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 723s 918ms/step - loss: 0.3749 - accuracy: 0.8236 - val_loss: 0.3116 - val_accuracy: 0.8681\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 711s 910ms/step - loss: 0.1879 - accuracy: 0.9293 - val_loss: 0.3386 - val_accuracy: 0.8664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f99caf23730>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "\n",
        "model.fit(x_train, y_train, epochs=2, batch_size=32, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caXODp3NPBDX",
        "outputId": "391b9ed3-abc3-4d81-bb88-d793a15d67c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 203s 259ms/step - loss: 0.3386 - accuracy: 0.8664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3386276960372925, 0.8663600087165833]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model.isTraining = False\n",
        "model.evaluate(x_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u46wb52DwqZZ",
        "outputId": "88cba3a2-e93a-443f-ed5f-b24ee6e2d464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  31 935  31\n",
            "    9 107]]\n",
            "1/1 [==============================] - 0s 346ms/step\n",
            "[[0.34911996 0.65088004]]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"i really was driven to the brink of falling asleep by this book\"\n",
        "s = \"this book illustrates how we can make a cake but is unable to talk about how to keep it overall does not solve the problem what a garbage\"\n",
        "s = \"from the beginning to the end there is virtually no clue what it is going to talk about\"\n",
        "s = \"i have no idea who will waste time to watch such a movie with no story at all and the words are awfully made up\"\n",
        "s = \"if you continue to make trouble i will beat you\"\n",
        "s = \"you are such a horrible man that can make trouble everywhere\"\n",
        "s = \"one plus one is two\"\n",
        "v = sentence_to_vector(s)\n",
        "v = np.expand_dims(v, 0)\n",
        "v = tf.keras.preprocessing.sequence.pad_sequences(v, maxlen=MAX_LEN)\n",
        "print(v)\n",
        "result = model.predict(v)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTqGxk0lFlg8",
        "outputId": "5ce22d39-2a28-4efc-d2e2-ce88bba1f061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " model_7 (Functional)           (None, 200, 64)      789888      ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 200, 64)     66368       ['model_7[0][0]',                \n",
            " eadAttention)                                                    'model_7[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 200, 64)      0           ['multi_head_attention_5[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 200, 64)     0           ['model_7[0][0]',                \n",
            " ambda)                                                           'dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 200, 64)     128         ['tf.__operators__.add_10[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 200, 64)      4160        ['layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 200, 64)      4160        ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 200, 64)      0           ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 200, 64)     0           ['layer_normalization_10[0][0]', \n",
            " ambda)                                                           'dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 200, 64)     128         ['tf.__operators__.add_11[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 64)          0           ['layer_normalization_11[0][0]'] \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 64)           0           ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 20000)        1300000     ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,164,832\n",
            "Trainable params: 1,374,944\n",
            "Non-trainable params: 789,888\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "False\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(8000, 200)\n",
            "(8000,)\n"
          ]
        }
      ],
      "source": [
        "def generator(model):\n",
        "  input = Input((MAX_LEN,))\n",
        "  temp_model = Model(model.input, model.layers[-4].output)\n",
        "  temp_output = temp_model(input)\n",
        "  \n",
        "  #lstm_output = LSTM(WORD_EMBED_DIM, dropout=0.25, recurrent_dropout=0.25)(temp_output)\n",
        "\n",
        "  lstm_output = transform(temp_output)\n",
        "  lstm_output = GlobalAveragePooling1D()(lstm_output)\n",
        "  lstm_output = Dropout(0.25)(lstm_output)\n",
        "  \n",
        "  next_output = Dense(VOCAB_SIZE, activation='softmax')(lstm_output)\n",
        "  next_model = Model(input, next_output)\n",
        "  next_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "  #for layer in next_model.layers:\n",
        "   # layer.trainable = False\n",
        "  next_model.layers[0].trainable = False\n",
        "\n",
        "  print(next_model.summary())\n",
        "\n",
        "  return next_model\n",
        "\n",
        "next_model = generator(model)\n",
        "for layer in next_model.layers:\n",
        "  print(layer.trainable)\n",
        "\n",
        "x_gen_train = []\n",
        "y_gen_train = []\n",
        "def go(wlist):\n",
        "  sublist = np.zeros((MAX_LEN,))\n",
        "  for i in range(0, 11):\n",
        "    sublist[i] = wlist[i]\n",
        "  for i in range(11, MAX_LEN):\n",
        "    sublist[i] = 0\n",
        "  x_gen_train.append(sublist)\n",
        "  y_gen_train.append(wlist[11])\n",
        "\n",
        "for i in range(8000):\n",
        "  go(x_train[i])\n",
        "\n",
        "x_gen_train = np.array(x_gen_train)\n",
        "y_gen_train = np.array(y_gen_train)\n",
        "print(x_gen_train.shape)\n",
        "print(y_gen_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN73wfLjMTV4",
        "outputId": "b53c37e8-20e4-4f58-ae2b-085a2b42b96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "250/250 [==============================] - 111s 442ms/step - loss: 6.0970\n",
            "Epoch 2/15\n",
            "250/250 [==============================] - 111s 442ms/step - loss: 6.0862\n",
            "Epoch 3/15\n",
            "250/250 [==============================] - 112s 446ms/step - loss: 6.0622\n",
            "Epoch 4/15\n",
            "250/250 [==============================] - 109s 438ms/step - loss: 6.0498\n",
            "Epoch 5/15\n",
            "250/250 [==============================] - 112s 446ms/step - loss: 6.0194\n",
            "Epoch 6/15\n",
            "250/250 [==============================] - 111s 443ms/step - loss: 5.9939\n",
            "Epoch 7/15\n",
            "250/250 [==============================] - 112s 446ms/step - loss: 5.9715\n",
            "Epoch 8/15\n",
            "250/250 [==============================] - 111s 442ms/step - loss: 5.9320\n",
            "Epoch 9/15\n",
            "250/250 [==============================] - 110s 441ms/step - loss: 5.8987\n",
            "Epoch 10/15\n",
            "250/250 [==============================] - 112s 449ms/step - loss: 5.8589\n",
            "Epoch 11/15\n",
            "250/250 [==============================] - 111s 446ms/step - loss: 5.8203\n",
            "Epoch 12/15\n",
            "250/250 [==============================] - 111s 446ms/step - loss: 5.7846\n",
            "Epoch 13/15\n",
            "250/250 [==============================] - 111s 444ms/step - loss: 5.7410\n",
            "Epoch 14/15\n",
            "250/250 [==============================] - 112s 450ms/step - loss: 5.7064\n",
            "Epoch 15/15\n",
            "250/250 [==============================] - 111s 444ms/step - loss: 5.6668\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fddcd8bb1f0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "next_model.fit(x_gen_train, y_gen_train, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtA6PuwlU5yC",
        "outputId": "fb4bc33c-42f8-4224-e590-abce78d9e3dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  14   20    9  368    7 1244    5  127   24   97]]\n",
            "1/1 [==============================] - 0s 254ms/step\n",
            "[[  14   20    9  368    7 1244    5  127   24   97   47]]\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "[[   14    20     9   368     7  1244     5   127    24    97    47 16519]]\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "[[   14    20     9   368     7  1244     5   127    24    97    47 16519\n",
            "    259]]\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "[[   14    20     9   368     7  1244     5   127    24    97    47 16519\n",
            "    259   140]]\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "[[   14    20     9   368     7  1244     5   127    24    97    47 16519\n",
            "    259   140    78]]\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "[[   14    20     9   368     7  1244     5   127    24    97    47 16519\n",
            "    259   140    78    15]]\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "[[   14    20     9   368     7  1244     5   127    24    97    47 16519\n",
            "    259   140    78    15    28]]\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "[[   14    20     9   368     7  1244     5   127    24    97    47 16519\n",
            "    259   140    78    15    28   823]]\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "[[   14    20     9   368     7  1244     5   127    24    97    47 16519\n",
            "    259   140    78    15    28   823    68]]\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "['this', 'movie', 'is', 'full', 'of', 'garbage', 'and', 'does', 'not', 'make', 'has', 'coccio', 'anyone', 'go', 'bad', 'that', 'have', 'crime', 'their', 'are']\n"
          ]
        }
      ],
      "source": [
        "def start(str):\n",
        "  words = str.split()\n",
        "  pwd = [wti[w] for w in words]\n",
        "\n",
        "  while len(pwd) < 20:\n",
        "    v = np.expand_dims(pwd, 0)\n",
        "    print(v)\n",
        "    v = tf.keras.preprocessing.sequence.pad_sequences(v, maxlen=MAX_LEN)\n",
        "    next_distribute = next_model.predict(v)[0]\n",
        "    next_index = np.random.choice(len(next_distribute), p=next_distribute)\n",
        "    #next_index = np.argmax(next_distribute)\n",
        "    pwd.append(next_index)\n",
        "\n",
        "  return pwd\n",
        "\n",
        "got = [itw[i] for i in start('this movie is full of garbage and does not make')]\n",
        "print(got)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdM/HDVoyLVHAp2WgD/k4e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}